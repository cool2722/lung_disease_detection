{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: NOT Recommended to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import logging\n",
    "import sys\n",
    "from ultralytics import YOLO\n",
    "import albumentations as A\n",
    "import random\n",
    "# from ultralytics.engine.callbacks import Callbacks\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path('/root')  #Path(\"/mnt/shared_dataset\")\n",
    "root = base / \"YOLO\"\n",
    "dicom_dir = base / \"physionet.org/files/vindr-cxr/1.0.0/train\"\n",
    "dicom_test_dir = base / \"physionet.org/files/vindr-cxr/1.0.0/test\"\n",
    "\n",
    "png_dir = root / \"images\"\n",
    "label_dir = root / \"labels\"\n",
    "test_dir = png_dir / \"test\"\n",
    "yaml_path = root / \"yolov12/my-yolov12.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in ['train', 'val']:\n",
    "    os.makedirs(os.path.join(png_dir,sub), exist_ok=True)\n",
    "    os.makedirs(os.path.join(label_dir, sub), exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = root / \"yolo_aggregated.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"image_path\"] = df[\"image_id\"].apply(lambda x: f\"YOLO/images/train/{x}.png\")  # adjust path/format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png_train = png_dir/\"train\"\n",
    "actual_pngs = {p.stem for p in (png_train).glob(\"*.png\")}\n",
    "\n",
    "# Compare with image IDs in dataframe (after No finding drop)\n",
    "df_ids = set(df[\"image_id\"])\n",
    "missing_png_ids = df_ids - actual_pngs\n",
    "log_dir = \"log_dir\"\n",
    "log_file = os.path.join(log_dir, \"missing_from_disk_after_drop.txt\")\n",
    "\n",
    "if missing_png_ids:\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    if not os.path.exists(log_file):\n",
    "        with open(log_file, \"w\") as f:\n",
    "            for mid in sorted(missing_png_ids):\n",
    "                f.write(mid + \"\\n\")\n",
    "    logging.warning(f\"{len(missing_png_ids)} image_ids missing as PNGs on disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class_id'] = df['class_name'].astype('category').cat.codes\n",
    "class_map = dict(enumerate(df['class_name'].astype('category').cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=5)\n",
    "df['fold'] = -1\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for fold, (_, val_idx) in enumerate(gkf.split(df, groups=df['image_id'])):\n",
    "    df.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "val_fold = 0  # 20% validation (1 out of 5 folds)\n",
    "\n",
    "df_ids = set(df[\"image_id\"])\n",
    "train_ids = set(df[df['fold'] != val_fold]['image_id'].apply(lambda x: Path(x).stem))\n",
    "val_ids = set(df[df['fold'] == val_fold]['image_id'].apply(lambda x: Path(x).stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"(Labels) LEN OF TRAIN DIR: {len(os.listdir('../labels/train'))}  |  LEN OF VAL DIR: {len(os.listdir('../labels/val'))}\")\n",
    "print(f\"üìä TRAIN SIZE: {len(train_ids)} | VAL SIZE: {len(val_ids)}\", file=sys.stderr)\n",
    "\n",
    "missing = []\n",
    "for name in val_ids:\n",
    "    path = os.path.join((label_dir/\"val\"), f\"{name}.txt\")\n",
    "    if not os.path.exists(path):\n",
    "        missing.append(name)\n",
    "\n",
    "# === Result\n",
    "if not missing:\n",
    "    print(\"‚úÖ All .txt files exist.\")\n",
    "else:\n",
    "    print(f\"‚ùå Missing {len(missing)} .txt files:\")\n",
    "    for m in missing:\n",
    "        print(\"  \", m)\n",
    "# === Log ===\n",
    "summary_path = \"log_dir/train_val_summary.txt\"\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(f\"TRAIN SIZE: {len(train_ids)}\\nVAL SIZE: {len(val_ids)}\\n\")\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üöÄ Starting augmentation\")\n",
    "\n",
    "albumentations_transform = A.Compose([\n",
    "    A.Rotate(limit=5, p=0.7),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=0, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.4),\n",
    "    A.MotionBlur(blur_limit=3, p=0.2),\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'], min_visibility=0.3))\n",
    "\n",
    "rare_class_names = [\n",
    "    'Clavicle fracture', 'Edema', 'Emphysema', 'Enlarged PA',\n",
    "    'Lung cavity', 'Lung cyst', 'Pneumothorax'\n",
    "]\n",
    "class_name_to_id = {v: k for k, v in class_map.items()}\n",
    "rare_class_ids = {class_name_to_id[c] for c in rare_class_names if c in class_name_to_id}\n",
    "\n",
    "for image_id, group in tqdm(df.groupby('image_id'), desc=\"üì∏ Augmenting images\"):\n",
    "    image_path = png_dir / \"train\" /f\"{image_id}.png\"\n",
    "    label_path = label_dir / \"train\" / f\"{image_id}.txt\"\n",
    "\n",
    "    print(f\"üîç Checking image_id: {image_id}\")\n",
    "\n",
    "    if not image_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Image missing: {image_path}\")\n",
    "        continue\n",
    "    if not label_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Label missing: {label_path}\")\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        print(f\"‚ö†Ô∏è  Failed to load image: {image_path}\")\n",
    "        continue\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    class_ids = group['class_id'].tolist()\n",
    "    bboxes = []\n",
    "    for _, row in group.iterrows():\n",
    "        x_center = (row.x_min + row.x_max) / 2 / w\n",
    "        y_center = (row.y_min + row.y_max) / 2 / h\n",
    "        box_w = (row.x_max - row.x_min) / w\n",
    "        box_h = (row.y_max - row.y_min) / h\n",
    "        bboxes.append([x_center, y_center, box_w, box_h])\n",
    "\n",
    "    is_rare = any(cls in rare_class_ids for cls in class_ids)\n",
    "    copies = 3 if is_rare else 2\n",
    "    probs = [0.2, 0.5, 0.9] if is_rare else [0.3, 0.8]\n",
    "\n",
    "    for i in range(copies):\n",
    "        aug_name = f\"aug_{image_id}_{i}\"\n",
    "        aug_img_path = png_dir / \"train\" / f\"{aug_name}.png\"\n",
    "        aug_lbl_path = label_dir / \"train\" / f\"{aug_name}.txt\"\n",
    "\n",
    "\n",
    "        if aug_img_path.exists() and aug_lbl_path.exists():\n",
    "            print(f\"‚è≠Ô∏è  Skipping existing: {aug_name}\")\n",
    "            continue\n",
    "\n",
    "        if random.random() < probs[i]:\n",
    "            try:\n",
    "                aug = albumentations_transform(image=img, bboxes=bboxes, class_labels=class_ids)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping {aug_name} due to bbox error: {e}\")\n",
    "                continue  # skip this augmented version\n",
    "\n",
    "            aug_img = aug['image']\n",
    "            aug_bboxes = aug['bboxes']\n",
    "            aug_classes = aug['class_labels']\n",
    "        else:\n",
    "            aug_img = img\n",
    "            aug_bboxes = bboxes\n",
    "            aug_classes = class_ids\n",
    "\n",
    "        print(f\"üìù Saving label to: {aug_lbl_path}\")\n",
    "        cv2.imwrite(str(aug_img_path), aug_img)\n",
    "        with open(aug_lbl_path, \"w\") as f:\n",
    "            for cls, box in zip(aug_classes, aug_bboxes):\n",
    "                f.write(f\"{cls} {box[0]:.6f} {box[1]:.6f} {box[2]:.6f} {box[3]:.6f}\\n\")\n",
    "\n",
    "        print(f\"‚úÖ Saved: {aug_img_path}\")\n",
    "\n",
    "print(\"üèÅ Finished augmenting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"# Lung Disease Dataset\n",
    "path: {root}\n",
    "train: images/train\n",
    "val: images/val\n",
    "test: images/test\n",
    "nc: {len(class_map)}\n",
    "names: {list(class_map.values())}\n",
    "\"\"\"\n",
    "print(yaml_content)\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Path(f\"{root}/yolov12/runs/detect/latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo12l.pt\")\n",
    "# print(f\"1 using {model}\")\n",
    "results = model.train(data = yaml_path, epochs=10, imgsz=640, batch = 16 ,device='cpu' , save = True, cache = True, project=run ,name=\"train\") # callbacks = callbacks,\n",
    "print(f\"2 done training. Results saved in: {results.save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_run = run / \"train\"\n",
    "weights_path = last_run / \"weights\" / \"best.pt\"\n",
    "if not weights_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Weights not found at {weights_path}\")\n",
    "\n",
    "print(f\"üß™ Testing first 100 .png images from: {weights_path}\")\n",
    "\n",
    "test_images = sorted(test_dir.glob(\"*.png\"))[:100]  # replace with your real test_dir\n",
    "model = YOLO(str(weights_path))\n",
    "results = model(\n",
    "    [str(p) for p in test_images],\n",
    "    save=True,\n",
    "    conf=0.25,\n",
    "    save_dir=f\"{run}/first_100_inference\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
